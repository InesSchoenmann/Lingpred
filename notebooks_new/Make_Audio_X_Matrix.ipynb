{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0adf827",
   "metadata": {},
   "source": [
    "# Making an X Matrix of the Mel-Spectrogramm of the audio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de7a9835",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib as plt\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b6c5a690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the data set and number of mels\n",
    "dataset = 'Goldstein'\n",
    "mels    = 8\n",
    "use_real_word_offsets = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "eca5e845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "if dataset== 'Goldstein':\n",
    "    acoustic_model = word_avg_sg(subject=1, session=1, n_mels=8, dataset = 'Goldstein',task = 1, use_real_word_offsets=True)\n",
    "    \n",
    "# otherwise we will have to loop over sessions (Armeni) or tasks (Gwilliams)\n",
    "# see code two cells down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "de6fdebb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5136, 9)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acoustic_model.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "76ab055b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "if dataset == 'Goldstein':\n",
    "\n",
    "    audio_dir = '/Users/ines/research/Lingpred/audio/'\n",
    "\n",
    "    if use_real_word_offsets:\n",
    "        filename  = audio_dir+'{}/acoustic_model_{}_mels_averaged_per_word_using_word_offsets.pkl'.format(dataset, mels)\n",
    "    else:\n",
    "        filename  = audio_dir+'{}/acoustic_model_{}_mels_averaged_per_word_using_next_word_onset_as_offset.pkl'.format(dataset, mels)\n",
    "    \n",
    "    f = open(filename,\"wb\")\n",
    "    pickle.dump(acoustic_model,f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c952a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject  = 3\n",
    "task     = 0  # uncomment for running for Armeni dataset\n",
    "#session = 0 # uncomment for running for Gwilliams dataset\n",
    "\n",
    "tasks    = ['0', '1', '2', '3']\n",
    "sessions = [1]\n",
    "mels     = [8]\n",
    "dataset = \"Armani\"\n",
    "\n",
    "# for the Gwilliams dataset we loop over tasks || for the Armani over sessions\n",
    "for mel in mels:\n",
    "    \n",
    "    acoustic_model = np.empty(shape=(0, mel+1))\n",
    "    \n",
    "    #for task in tasks: \n",
    "    for session in sessions: # uncomment for running for Armeni dataset\n",
    "        \n",
    "        if subject == 3 and dataset == 'Armani':\n",
    "            if session == 8:\n",
    "                continue\n",
    "        \n",
    "        avg_spec_init_phone = word_avg_sg(subject=subject, session=session, \n",
    "                                                  n_mels=mel, dataset = dataset,\n",
    "                                                  task = task)\n",
    "        print(avg_spec_init_phone.shape)\n",
    "        acoustic_model = np.vstack([acoustic_model, avg_spec_init_phone])\n",
    "\n",
    "    filename = '/project/3018059.03/data/{}/acoustic_model_{}_mels_averaged_per_word.pkl'.format(dataset, mel)\n",
    "    \n",
    "    if subject == 3 and dataset == 'Armani':\n",
    "        filename = '/project/3018059.03/data/{}/acoustic_model_without_session_8_{}_mels_averaged_per_word.pkl'.format(dataset, mel)\n",
    "    f = open(filename,\"wb\")\n",
    "    pickle.dump(acoustic_model,f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0fac261d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_avg_sg(subject:int, session:int, task= '0', dataset='Armani', n_mels=8, use_real_word_offsets=True):\n",
    "    '''\n",
    "    Params:\n",
    "    -------\n",
    "    - Dataset: 'Armani' or Gwilliams\n",
    "    - Subject\n",
    "    - Session\n",
    "    - n_mels: nr of bands for the mel spectogram\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Array of shape (nr_words, n_mels+1)\n",
    "    containing the spectogram information averaged over time for each mel band \n",
    "    '''\n",
    "    avg_sg_all_runs = np.empty(shape=(0, n_mels+1)) # +1 for the variance \n",
    "    \n",
    "    if dataset == 'Armani':\n",
    "        audio_dir = '/project/3018059.03/data/Armani/stimuli/'\n",
    "        # sampling rate MEG data:\n",
    "        sr_meg = 1200\n",
    "        \n",
    "    if dataset == 'Gwilliams':\n",
    "        audio_dir = '/project/3018059.03/data/Gwilliams/'\n",
    "        # sampling rate MEG data:\n",
    "        sr_meg = 1000\n",
    "\n",
    "    if dataset=='Goldstein':\n",
    "        audio_dir = '/Users/ines/research/Lingpred/audio/Goldstein/'\n",
    "        sr_meg    = 512\n",
    "        \n",
    "    # get runs in this session\n",
    "    runs = get_runs(dataset, session, subject, task)\n",
    "\n",
    "    \n",
    "    for run in runs:\n",
    "        \n",
    "        print(run)\n",
    "        \n",
    "        # get onset times of the initial phonemes adjusted to onset of the audiofile:\n",
    "        df_words = get_words_onsets_offsets(dataset, subject, session, run, task, use_real_word_offsets=use_real_word_offsets)\n",
    "        \n",
    "        \n",
    "        # audio file name \n",
    "        if session<10:\n",
    "            audio_run = audio_dir + '0{}_{}.wav'.format(session, run)\n",
    "        else:\n",
    "            audio_run = audio_dir + '{}_{}.wav'.format(session, run)\n",
    "            \n",
    "        if dataset == 'Gwilliams':\n",
    "            audio_run = audio_dir + df_words.sound.unique()[0]\n",
    "\n",
    "        if dataset == 'Goldstein':\n",
    "            audio_run = audio_dir + 'monkey_and_horse_corrected.wav'\n",
    "\n",
    "        # waveform (scale) and sampling rate (sr)\n",
    "        scale, sr = librosa.load(audio_run, sr=sr_meg*18)\n",
    "\n",
    "        # make spectrogram \n",
    "        mel_sg   = librosa.feature.melspectrogram(y=scale, sr=sr, hop_length=int(sr/sr_meg), \n",
    "                                                  n_mels=n_mels)\n",
    "        lm_sg    = librosa.power_to_db(mel_sg)\n",
    "        lm_time  = np.arange(1,lm_sg.shape[1]+1)/sr_meg\n",
    "        \n",
    "        # average over the duration of the phoneme for each band:\n",
    "        # resulting array is of shape (nr_phonemes, nr_bands)\n",
    "        discrete_events = discretise_events(lm_sg, lm_time, onset_df=df_words)\n",
    "        \n",
    "        # stack for all runs:\n",
    "        avg_sg_all_runs = np.vstack((avg_sg_all_runs, discrete_events))\n",
    "        \n",
    "    return avg_sg_all_runs\n",
    "\n",
    "def make_acoustic_y_matrix(subject:int, session:int, task= '0', dataset='Armani', n_mels=8,\n",
    "                        only_word_inital_phonemes=True):\n",
    "    ''''\n",
    "    Creates a Mel Spectogram with n_mels + the envelope and that resembles the shape of the neural data. \n",
    "    Hence, it will have the shape (n_mels+1, n_words, n_timepoints)\n",
    "    '''\n",
    "    if dataset == 'Armani':\n",
    "        audio_dir = '/project/3018059.03/data/Armani/stimuli/'\n",
    "        # sampling rate MEG data:\n",
    "        sr_meg = 1200\n",
    "        \n",
    "    if dataset == 'Gwilliams':\n",
    "        audio_dir = '/project/3018059.03/data/Gwilliams/'\n",
    "        # sampling rate MEG data:\n",
    "        sr_meg = 1000\n",
    "    \n",
    "    y_matrix_all_runs = np.empty(shape=(0, n_mels+1, sr_meg*4)) # +1 for the variance \n",
    "    \n",
    "    # get runs in this session\n",
    "    runs = get_runs(dataset, session, subject, task)\n",
    "\n",
    "    for run in runs:\n",
    "        \n",
    "        print(run)\n",
    "        # get onset times of the initial phonemes adjusted to onset of the audiofile:\n",
    "        df_phonemes = get_phonemes_onsets_offsets(dataset, subject, session, run, task, only_word_inital_phonemes)\n",
    "        \n",
    "        # audio file name \n",
    "        if session<10:\n",
    "            audio_run = audio_dir + '0{}_{}.wav'.format(session, run)\n",
    "        else:\n",
    "            audio_run = audio_dir + '{}_{}.wav'.format(session, run)\n",
    "            \n",
    "        if dataset == 'Gwilliams':\n",
    "            audio_run = audio_dir + df_phonemes.sound.unique()[0]\n",
    "\n",
    "        # waveform (scale) and sampling rate (sr)\n",
    "        scale, sr = librosa.load(audio_run, sr=sr_meg*18)\n",
    "\n",
    "        # make spectrogram \n",
    "        mel_sg   = librosa.feature.melspectrogram(y=scale, sr=sr, hop_length=int(sr/sr_meg), \n",
    "                                                  n_mels=n_mels)\n",
    "        lm_sg    = librosa.power_to_db(mel_sg)\n",
    "        lm_time  = np.arange(1,lm_sg.shape[1]+1)/sr_meg\n",
    "\n",
    "        # average over the duration of the phoneme for each band:\n",
    "        # resulting array is of shape (nr_epochs, nr_bands+1, nr_timepoints)\n",
    "        epochs = epoch_events(lm_sg, lm_time, onset_df=df_phonemes, sr_meg=sr_meg)\n",
    "        \n",
    "        # stack for all runs:\n",
    "        y_matrix_all_runs = np.vstack((y_matrix_all_runs, epochs))\n",
    "        \n",
    "    return y_matrix_all_runs\n",
    "\n",
    "def init_phoneme_avg_sg(subject:int, session:int, task= '0', dataset='Armani', n_mels=128, \n",
    "                        only_word_inital_phonemes=True):\n",
    "    '''\n",
    "    Params:\n",
    "    -------\n",
    "    - Dataset: 'Armani' or Gwilliams\n",
    "    - Subject\n",
    "    - Session\n",
    "    - n_mels: nr of bands for the mel spectogram\n",
    "    - power\n",
    "    - only_word_inital_phonemes: whether or not only to consider word-inital phonemes\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Array of shape (nr_initial_phonemes, n_mels)\n",
    "    containing the spectogram information averaged over time for each mel band \n",
    "    '''\n",
    "    avg_sg_all_runs = np.empty(shape=(0, n_mels+1)) # +1 for the variance \n",
    "    \n",
    "    if dataset == 'Armani':\n",
    "        audio_dir = '/project/3018059.03/data/Armani/stimuli/'\n",
    "        # sampling rate MEG data:\n",
    "        sr_meg = 1200\n",
    "        \n",
    "    if dataset == 'Gwilliams':\n",
    "        audio_dir = '/project/3018059.03/data/Gwilliams/'\n",
    "        # sampling rate MEG data:\n",
    "        sr_meg = 1000\n",
    "        \n",
    "    # get runs in this session\n",
    "    runs = get_runs(dataset, session, subject, task)\n",
    "\n",
    "    \n",
    "    for run in runs:\n",
    "        \n",
    "        print(run)\n",
    "        \n",
    "        # get onset times of the initial phonemes adjusted to onset of the audiofile:\n",
    "        df_phonemes = get_phonemes_onsets_offsets(dataset, subject, session, run, task, only_word_inital_phonemes)\n",
    "        \n",
    "        \n",
    "        # audio file name \n",
    "        if session<10:\n",
    "            audio_run = audio_dir + '0{}_{}.wav'.format(session, run)\n",
    "        else:\n",
    "            audio_run = audio_dir + '{}_{}.wav'.format(session, run)\n",
    "            \n",
    "        if dataset == 'Gwilliams':\n",
    "            audio_run = audio_dir + df_phonemes.sound.unique()[0]\n",
    "\n",
    "        # waveform (scale) and sampling rate (sr)\n",
    "        scale, sr = librosa.load(audio_run, sr=sr_meg*18)\n",
    "\n",
    "        # make spectrogram \n",
    "        mel_sg   = librosa.feature.melspectrogram(y=scale, sr=sr, hop_length=int(sr/sr_meg), \n",
    "                                                  n_mels=n_mels)\n",
    "        lm_sg    = librosa.power_to_db(mel_sg)\n",
    "        lm_time  = np.arange(1,lm_sg.shape[1]+1)/sr_meg\n",
    "        \n",
    "        # average over the duration of the phoneme for each band:\n",
    "        # resulting array is of shape (nr_phonemes, nr_bands)\n",
    "        discrete_events = discretise_events(lm_sg, lm_time, onset_df=df_phonemes)\n",
    "        \n",
    "        # stack for all runs:\n",
    "        avg_sg_all_runs = np.vstack((avg_sg_all_runs, discrete_events))\n",
    "        \n",
    "    return avg_sg_all_runs\n",
    "\n",
    "def epoch_events(spectogram, times, onset_df, sr_meg):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    - spectogram: librosa mel spectrogram in db\n",
    "        spectrogram of a given run, of shape (n_mels, sampling_rate*seconds) with sampling_rate = MEG_sr *18\n",
    "    - times: numpy array\n",
    "        containing the times in seconds corresponding to each time point in the spectogram\n",
    "    - onset_df: pandas DataFrame\n",
    "        containing a column 'onset' and 'offset' indicating the onset of each word-initial phoneme in seconds\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    - numpy ndarray of shape (n_epochs, n_mels+1, n_timepoints) with n_timepoints = 4s * MEG_sampling_rate\n",
    "        containing the envelope and the mels for each epoch\n",
    "    '''\n",
    "\n",
    "    audio={'audio':spectogram,'times':times}\n",
    "        \n",
    "    # np array of shape (onsets, bands, time_points) \n",
    "    bands = np.zeros((len(onset_df),audio['audio'].shape[0], sr_meg*4))\n",
    "    var   = np.zeros(shape=(len(onset_df), sr_meg*4))\n",
    "\n",
    "    for ph_i,ph_row in enumerate(onset_df.iterrows()):\n",
    "            \n",
    "        # make a logical array for audio timepoints that are part of the epoch [onset-2s, onset+2s):\n",
    "        samps_ix=np.logical_and(audio['times']>ph_row[1]['onset']-2,\n",
    "                                audio['times']<ph_row[1]['onset']+2)\n",
    "        \n",
    "        # get samples for this epoch:\n",
    "        temp = audio['audio'][:,samps_ix]\n",
    "\n",
    "        # check if there are the right amount of samples for the epoch, i.e. 4s * MEG_sampling rate\n",
    "        # and pad with zeros either to the left (beginning of audio run) or right (end of audio run)\n",
    "        if temp.shape[1] < sr_meg*4:\n",
    "            if ph_i < len(onset_df)/2: \n",
    "                temp = pad_left(temp, sr_meg*4)\n",
    "            else:\n",
    "                temp = pad_right(temp, sr_meg*4)\n",
    "        \n",
    "        bands[ph_i]= temp\n",
    "\n",
    "    # compute variance over bands for each time point:\n",
    "    var = np.var(bands, axis=1)\n",
    "\n",
    "    # swap axis so mels are dimension 0  \n",
    "    bands = np.swapaxes(bands, 0, 1)\n",
    "\n",
    "    # make array to hold variance and bands, has shape (n_mels+1, n_epochs, n_timepoints)\n",
    "    band_stats     = np.zeros(shape=(bands.shape[0]+1, bands.shape[1], bands.shape[2]))\n",
    "    band_stats[0]  = var\n",
    "    band_stats[1:] = bands\n",
    "        \n",
    "    # scrub nans \n",
    "    band_stats[np.isnan(band_stats)]=0\n",
    "\n",
    "    # swap axes again to have epochs first ... easier for stacking:\n",
    "    band_stats = np.swapaxes(band_stats, 0, 1)\n",
    "    \n",
    "    return(band_stats)\n",
    "\n",
    "def get_phonemes_onsets_offsets(dataset:str, subject:int, session:int, run:int, task='0',\n",
    "                                only_word_inital_phonemes=True):\n",
    "    '''\n",
    "    Params:\n",
    "    - raw_data object\n",
    "    - dataset: dataset for which the offsets are supposed to be loaded: Gwilliams, Armani or sherlock\n",
    "    - subject: subject for which the offsets are supposed to be loaded\n",
    "    - session: session for which the offsets are supposed to be loaded\n",
    "    - run: run for which the offsets are supposed to be loaded\n",
    "    - only_word_inital_phonemes: whether or not to get only word-inital phonemes\n",
    "    \n",
    "    Returns:\n",
    "    - pandas data frame with at least with 3 columns: phoneme, onset  \n",
    "    \n",
    "    '''\n",
    "    if dataset == 'Armani':\n",
    "        \n",
    "        # handle naming of session 10: \n",
    "        if session < 10:\n",
    "            sess = '00' + str(session)\n",
    "        else: \n",
    "            sess = '0' + str(session)\n",
    "        \n",
    "        # get path to events file:\n",
    "        dir_path = '/project/3018059.03/data/Armani/'\n",
    "        filepath = 'sub-00' + str(subject) +'/' + 'ses-' + sess +'/'+ 'meg/'\n",
    "        filename = 'sub-00' + str(subject) + '_ses-' + sess + '_task-compr_events.tsv'\n",
    "        \n",
    "        # read pandas DataFrame for the entire session:\n",
    "        annotations = pd.read_csv(dir_path+filepath+filename, sep='\\t')\n",
    "        \n",
    "        # get list with word_onsets for this run:\n",
    "        word_onset_name = ['word_onset_0{}'.format(run)]\n",
    "        df_words        = annotations[annotations.type.isin(word_onset_name)]\n",
    "        df_words        = df_words[df_words.value != 'sp']\n",
    "        word_onsets     = df_words.onset\n",
    "        \n",
    "        # now look for the timing of the audio onset for this run:\n",
    "        index_first_word = df_words.index[0] # index for the first word in this run\n",
    "        \n",
    "        for i in np.arange(index_first_word, -1, -1):     # interate from there backwards\n",
    "            if annotations.iloc[i].type == 'wav_onset':   # to the most recent wave onset\n",
    "                audio_onset = annotations.iloc[i].onset   # and get it's onset time\n",
    "                break                                     # break out of for loop\n",
    "        \n",
    "        # get list with phoneme_onsets for this run::\n",
    "        onset_name = ['phoneme_onset_0{}'.format(run)]\n",
    "        \n",
    "        # get only phonemes and clean data frame\n",
    "        df_phonemes = annotations[annotations.type.isin(onset_name)]\n",
    "        \n",
    "        if only_word_inital_phonemes:\n",
    "            df_phonemes = df_phonemes[df_phonemes.onset.isin(word_onsets)]\n",
    "        else: df_phonemes = df_phonemes[df_phonemes.value != 'sp']\n",
    "        \n",
    "        #convert times to audio onset times:\n",
    "        df_phonemes.onset = df_phonemes.onset - audio_onset\n",
    "        \n",
    "        # add a column with the offsets:\n",
    "        offsets               = df_phonemes.onset + df_phonemes.duration\n",
    "        df_phonemes['offset'] = offsets\n",
    "        \n",
    "    if dataset == 'Gwilliams':\n",
    "        \n",
    "        path_dir = '/project/3018059.03/data/Gwilliams/'\n",
    "        file_name = 'annotation_task_'+ task + '.tsv'\n",
    "        \n",
    "        # read pandas DataFrame and keep only sentences (not word lists):\n",
    "        annotations = pd.read_csv(path_dir+file_name, sep='\\t')\n",
    "        annotations = annotations[annotations['condition']=='sentence']\n",
    "        \n",
    "        # keep only this run\n",
    "        story_id    = [float(run)]\n",
    "        df_story    = annotations[annotations.sound_id.isin(story_id)]\n",
    "        \n",
    "        # get list with word_onsets for this run (i.e. story uid):\n",
    "        df_words    = df_story[df_story['kind']=='word']\n",
    "        word_onsets = df_words.start\n",
    "        \n",
    "        # get only phonemes \n",
    "        df_phonemes = df_story[df_story['kind']=='phoneme']\n",
    "        \n",
    "        #re-name start column:\n",
    "        df_phonemes['onset'] = df_phonemes.start\n",
    "        \n",
    "        # add a column with the offsets:\n",
    "        offsets               = df_phonemes.onset[1:].to_list()+[0.08]\n",
    "        df_phonemes['offset'] = offsets\n",
    "        \n",
    "        # and now only keep the word_initial phonemes:\n",
    "        if only_word_inital_phonemes:\n",
    "            df_phonemes = df_phonemes[df_phonemes.start.isin(word_onsets)]\n",
    "    \n",
    "    \n",
    "    return df_phonemes\n",
    "\n",
    "\n",
    "def get_runs(dataset, session, subject, task):\n",
    "    \n",
    "    if dataset == 'Armani':\n",
    "        runs = ASH_runs_in_session(session,subject)\n",
    "        \n",
    "    if dataset == 'Gwilliams':\n",
    "        if task == '0':\n",
    "            runs = np.arange(0, 4)\n",
    "        if task == '1':\n",
    "            runs = np.arange(0, 6)\n",
    "        if task == '2':\n",
    "            runs = np.arange(0, 8)\n",
    "        if task == '3':\n",
    "            runs = np.arange(0, 12)\n",
    "    \n",
    "    if dataset=='Goldstein':\n",
    "        runs = [0]\n",
    "    return runs \n",
    "\n",
    "def discretise_events(spectogram, times, onset_df):\n",
    "\n",
    "    # make dictionary of spectogram and meg sampled time points \n",
    "    audio={'audio':spectogram,'times':times}\n",
    "    \n",
    "    # np array of shape (onsets, bands) for the stats of each band \n",
    "    band_means = np.zeros((len(onset_df),audio['audio'].shape[0]))\n",
    "    var        = np.zeros(shape=(len(onset_df)))\n",
    "\n",
    "    for ph_i,ph_row in enumerate(onset_df.iterrows()):\n",
    "        \n",
    "        # make a logical array for audio timepoints that are part of the phoneme:\n",
    "        samps_ix=np.logical_and(audio['times']>ph_row[1]['onset'],\n",
    "                            audio['times']<ph_row[1]['offset'])\n",
    "        \n",
    "        # Average & variance over these timepoints for each band:\n",
    "        band_means[ph_i,:]= np.mean(audio['audio'][:,samps_ix],axis=1)\n",
    "        var[ph_i]         = np.var(audio['audio'][:,samps_ix])\n",
    "        \n",
    "        # Add as columns\n",
    "        band_stats = np.c_[band_means, var]\n",
    "        \n",
    "    # scrub nans \n",
    "    band_stats[np.isnan(band_stats)]=0\n",
    "    \n",
    "    return(band_stats)\n",
    "\n",
    "def get_words_onsets_offsets(dataset:str, subject:int, session:int, run:int, task='0', use_real_word_offsets=True):\n",
    "    '''\n",
    "    Params:\n",
    "    - raw_data object\n",
    "    - dataset: dataset for which the offsets are supposed to be loaded: Gwilliams, Armani or sherlock\n",
    "    - subject: subject for which the offsets are supposed to be loaded\n",
    "    - session: session for which the offsets are supposed to be loaded\n",
    "    - run: run for which the offsets are supposed to be loaded\n",
    "    \n",
    "    Returns:\n",
    "    - pandas data frame with at least with 3 columns: word, onset, offset \n",
    "    \n",
    "    '''\n",
    "    if dataset == 'Armani':\n",
    "        \n",
    "        # handle naming of session 10: \n",
    "        if session < 10:\n",
    "            sess = '00' + str(session)\n",
    "        else: \n",
    "            sess = '0' + str(session)\n",
    "        \n",
    "        # get path to events file:\n",
    "        dir_path = '/project/3018059.03/data/Armani/'\n",
    "        filepath = 'sub-00' + str(subject) +'/' + 'ses-' + sess +'/'+ 'meg/'\n",
    "        filename = 'sub-00' + str(subject) + '_ses-' + sess + '_task-compr_events.tsv'\n",
    "        \n",
    "        # read pandas DataFrame for the entire session:\n",
    "        annotations = pd.read_csv(dir_path+filepath+filename, sep='\\t')\n",
    "        \n",
    "        # get list with word_onsets for this run:\n",
    "        word_onset_name = ['word_onset_0{}'.format(run)]\n",
    "        df_words        = annotations[annotations.type.isin(word_onset_name)]\n",
    "        df_words        = df_words[df_words.value != 'sp']\n",
    "        word_onsets     = df_words.onset\n",
    "        \n",
    "        # now look for the timing of the audio onset for this run:\n",
    "        index_first_word = df_words.index[0] # index for the first word in this run\n",
    "        \n",
    "        for i in np.arange(index_first_word, -1, -1):     # interate from there backwards\n",
    "            if annotations.iloc[i].type == 'wav_onset':   # to the most recent wave onset\n",
    "                audio_onset = annotations.iloc[i].onset   # and get it's onset time\n",
    "                break                                     # break out of for loop\n",
    "        \n",
    "        #convert times to audio onset times:\n",
    "        df_words.onset = df_words.onset - audio_onset\n",
    "        \n",
    "        # add a column with the offsets:\n",
    "        offsets               = df_words.onset + df_words.duration\n",
    "        df_words['offset'] = offsets\n",
    "        \n",
    "    if dataset == 'Gwilliams':\n",
    "        \n",
    "        path_dir = '/project/3018059.03/data/Gwilliams/'\n",
    "        file_name = 'annotation_task_'+ task + '.tsv'\n",
    "        \n",
    "        # read pandas DataFrame and keep only sentences (not word lists):\n",
    "        annotations = pd.read_csv(path_dir+file_name, sep='\\t')\n",
    "        annotations = annotations[annotations['condition']=='sentence']\n",
    "        \n",
    "        # keep only this run\n",
    "        story_id    = [float(run)]\n",
    "        df_story    = annotations[annotations.sound_id.isin(story_id)]\n",
    "        \n",
    "        # get list with word_onsets for this run (i.e. story uid):\n",
    "        df_words    = df_story[df_story['kind']=='word']\n",
    "        \n",
    "        #re-name start column:\n",
    "        df_words.rename(columns={'start': 'onset'}, inplace=True)\n",
    "        \n",
    "        # add a column with the offsets:\n",
    "        offsets            = df_words.onset[1:].to_list()+[df_words.onset.iloc[-1] + 0.08]\n",
    "        df_words['offset'] = offsets\n",
    "        \n",
    "    if dataset == 'Goldstein':\n",
    "        \n",
    "        dir_path  = '/Users/ines/research/Lingpred/audio/Goldstein/'\n",
    "        file_name =  'podcast_transcript.csv'\n",
    "        filepath  = dir_path + file_name\n",
    "\n",
    "        df_words = pd.read_csv(filepath, sep=',')\n",
    "        df_words.rename(columns={'start': 'onset'}, inplace=True)\n",
    "\n",
    "        if not use_real_word_offsets:\n",
    "            # add a column with the offsets:\n",
    "            offsets            = [x - 0.005 for x in df_words.onset[1:].to_list()]+[df_words.end.iloc[-1]]\n",
    "            df_words['offset'] = offsets\n",
    "\n",
    "            #sometimes the two people talk over one another, to avoid negative new_offset times I will replace them with the value in 'end'\n",
    "            df_words['offset'] = np.where(df_words[\"offset\"] < df_words[\"onset\"], df_words[\"end\"], df_words[\"offset\"])\n",
    "\n",
    "        else:\n",
    "            df_words.rename(columns={'start': 'onset', 'end':'offset'}, inplace=True)\n",
    "\n",
    "    return df_words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b02b9c46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>onset</th>\n",
       "      <th>end</th>\n",
       "      <th>offset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Act</td>\n",
       "      <td>3.710</td>\n",
       "      <td>3.790</td>\n",
       "      <td>3.985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>one,</td>\n",
       "      <td>3.990</td>\n",
       "      <td>4.190</td>\n",
       "      <td>4.646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>monkey</td>\n",
       "      <td>4.651</td>\n",
       "      <td>4.931</td>\n",
       "      <td>4.946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>in</td>\n",
       "      <td>4.951</td>\n",
       "      <td>5.011</td>\n",
       "      <td>5.046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the</td>\n",
       "      <td>5.051</td>\n",
       "      <td>5.111</td>\n",
       "      <td>5.146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5131</th>\n",
       "      <td>go</td>\n",
       "      <td>1798.546</td>\n",
       "      <td>1798.646</td>\n",
       "      <td>1798.661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5132</th>\n",
       "      <td>to</td>\n",
       "      <td>1798.666</td>\n",
       "      <td>1798.746</td>\n",
       "      <td>1798.781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5133</th>\n",
       "      <td>court</td>\n",
       "      <td>1798.786</td>\n",
       "      <td>1799.006</td>\n",
       "      <td>1799.041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5134</th>\n",
       "      <td>over</td>\n",
       "      <td>1799.046</td>\n",
       "      <td>1799.226</td>\n",
       "      <td>1799.322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5135</th>\n",
       "      <td>it.</td>\n",
       "      <td>1799.327</td>\n",
       "      <td>1799.367</td>\n",
       "      <td>1799.367</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5136 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        word     onset       end    offset\n",
       "0        Act     3.710     3.790     3.985\n",
       "1       one,     3.990     4.190     4.646\n",
       "2     monkey     4.651     4.931     4.946\n",
       "3         in     4.951     5.011     5.046\n",
       "4        the     5.051     5.111     5.146\n",
       "...      ...       ...       ...       ...\n",
       "5131      go  1798.546  1798.646  1798.661\n",
       "5132      to  1798.666  1798.746  1798.781\n",
       "5133   court  1798.786  1799.006  1799.041\n",
       "5134    over  1799.046  1799.226  1799.322\n",
       "5135     it.  1799.327  1799.367  1799.367\n",
       "\n",
       "[5136 rows x 4 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_words_onsets_offsets(dataset='Goldstein', subject=1, session=1, run=1, task='0', use_real_word_offsets=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad17460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11298410720582895 -2.783900000000017 1669\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>onset</th>\n",
       "      <th>offset</th>\n",
       "      <th>new_offset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1660</th>\n",
       "      <td>by</td>\n",
       "      <td>566.969828</td>\n",
       "      <td>567.139828</td>\n",
       "      <td>567.174828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1661</th>\n",
       "      <td>a</td>\n",
       "      <td>567.179828</td>\n",
       "      <td>567.209828</td>\n",
       "      <td>567.304828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1662</th>\n",
       "      <td>monkey</td>\n",
       "      <td>567.309828</td>\n",
       "      <td>567.739828</td>\n",
       "      <td>568.134828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1663</th>\n",
       "      <td>Yeah</td>\n",
       "      <td>568.139828</td>\n",
       "      <td>568.359828</td>\n",
       "      <td>568.694828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1664</th>\n",
       "      <td>just</td>\n",
       "      <td>568.699828</td>\n",
       "      <td>568.829828</td>\n",
       "      <td>568.924828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1665</th>\n",
       "      <td>as</td>\n",
       "      <td>568.929828</td>\n",
       "      <td>568.979828</td>\n",
       "      <td>569.014828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1666</th>\n",
       "      <td>I</td>\n",
       "      <td>569.019828</td>\n",
       "      <td>569.129828</td>\n",
       "      <td>569.224828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1667</th>\n",
       "      <td>just</td>\n",
       "      <td>569.229828</td>\n",
       "      <td>569.399828</td>\n",
       "      <td>569.934828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1668</th>\n",
       "      <td>yeah</td>\n",
       "      <td>569.939828</td>\n",
       "      <td>570.089828</td>\n",
       "      <td>570.694828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1669</th>\n",
       "      <td>Yeah</td>\n",
       "      <td>570.699828</td>\n",
       "      <td>570.839828</td>\n",
       "      <td>568.055928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1670</th>\n",
       "      <td>You</td>\n",
       "      <td>568.060928</td>\n",
       "      <td>568.120928</td>\n",
       "      <td>568.155928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1671</th>\n",
       "      <td>found</td>\n",
       "      <td>568.160928</td>\n",
       "      <td>568.340928</td>\n",
       "      <td>568.395928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1672</th>\n",
       "      <td>out</td>\n",
       "      <td>568.400928</td>\n",
       "      <td>568.530928</td>\n",
       "      <td>568.545928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1673</th>\n",
       "      <td>the</td>\n",
       "      <td>568.550928</td>\n",
       "      <td>568.610928</td>\n",
       "      <td>568.665928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674</th>\n",
       "      <td>monkey</td>\n",
       "      <td>568.670928</td>\n",
       "      <td>568.940928</td>\n",
       "      <td>568.955928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1675</th>\n",
       "      <td>was</td>\n",
       "      <td>568.960928</td>\n",
       "      <td>569.130928</td>\n",
       "      <td>569.185928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1676</th>\n",
       "      <td>suing</td>\n",
       "      <td>569.190928</td>\n",
       "      <td>569.450928</td>\n",
       "      <td>569.485928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1677</th>\n",
       "      <td>you</td>\n",
       "      <td>569.490928</td>\n",
       "      <td>569.760928</td>\n",
       "      <td>569.955928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1678</th>\n",
       "      <td>because</td>\n",
       "      <td>569.960928</td>\n",
       "      <td>570.240928</td>\n",
       "      <td>570.295928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1679</th>\n",
       "      <td>your</td>\n",
       "      <td>570.300928</td>\n",
       "      <td>570.390928</td>\n",
       "      <td>570.425928</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         word       onset      offset  new_offset\n",
       "1660       by  566.969828  567.139828  567.174828\n",
       "1661        a  567.179828  567.209828  567.304828\n",
       "1662   monkey  567.309828  567.739828  568.134828\n",
       "1663     Yeah  568.139828  568.359828  568.694828\n",
       "1664     just  568.699828  568.829828  568.924828\n",
       "1665       as  568.929828  568.979828  569.014828\n",
       "1666        I  569.019828  569.129828  569.224828\n",
       "1667     just  569.229828  569.399828  569.934828\n",
       "1668     yeah  569.939828  570.089828  570.694828\n",
       "1669     Yeah  570.699828  570.839828  568.055928\n",
       "1670      You  568.060928  568.120928  568.155928\n",
       "1671    found  568.160928  568.340928  568.395928\n",
       "1672      out  568.400928  568.530928  568.545928\n",
       "1673      the  568.550928  568.610928  568.665928\n",
       "1674   monkey  568.670928  568.940928  568.955928\n",
       "1675      was  568.960928  569.130928  569.185928\n",
       "1676    suing  569.190928  569.450928  569.485928\n",
       "1677      you  569.490928  569.760928  569.955928\n",
       "1678  because  569.960928  570.240928  570.295928\n",
       "1679     your  570.300928  570.390928  570.425928"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#diff = df.offset - df.end\n",
    "#print(diff.mean(), diff.min(), diff.argmin())\n",
    "#df.iloc[1660:1680]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ef08e4",
   "metadata": {},
   "source": [
    "### Ok, this is happening because they are talking over one another here. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
